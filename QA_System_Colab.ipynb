{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Deep Learning Question Answering System\n\nThis notebook helps you run the QA system on Google Colab."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Clone the repository\n!git clone https://github.com/vedant7001/DeepLearningProject.git\n%cd DeepLearningProject"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Install dependencies\n!pip install torch transformers numpy tqdm tensorboard pandas scikit-learn matplotlib"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Setup Python path properly\nimport sys\nimport os\n\n# Get current working directory\n!pwd\n\n# Add the current directory to the Python path\nsys.path.insert(0, os.getcwd())\n\n# Create empty __init__.py files to make directories importable\n!touch __init__.py\n!touch training/__init__.py\n!touch models/__init__.py\n!touch utils/__init__.py\n\n# Verify paths\nprint(f\"Python path: {sys.path}\")\nprint(f\"Directory contents: {os.listdir()}\")\nprint(f\"Training directory contents: {os.listdir('training')}\")\n\n# Verify GPU\nimport torch\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Configuration"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Training configuration\nconfig = {\n    'model_type': 'lstm',  # Options: 'lstm', 'attn', 'transformer'\n    'batch_size': 16,\n    'num_epochs': 20,\n    'embedding_size': 128,\n    'hidden_size': 256,\n    'learning_rate': 3e-4,\n    'dropout': 0.3,\n    'num_layers': 2\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Direct Import Test"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Test imports directly\ntry:\n    import training\n    print(\"\u2705 Successfully imported training module\")\n    print(f\"Training module path: {training.__file__}\")\n    \n    import training.train\n    print(\"\u2705 Successfully imported training.train module\")\n    print(f\"Training.train module path: {training.train.__file__}\")\n    \n    from training.train import train\n    print(\"\u2705 Successfully imported train function\")\n    \n    import models\n    print(\"\u2705 Successfully imported models module\")\n    \n    import utils\n    print(\"\u2705 Successfully imported utils module\")\nexcept Exception as e:\n    print(f\"\u274c Import failed: {e}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Training"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Import and run training\ntry:\n    from training.train import train\n\n    # Create output directory\n    output_dir = 'Result/model_outputs'\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Start training\n    train(\n        model_type=config['model_type'],\n        tokenizer_name='bert-base-uncased',\n        output_dir=output_dir,\n        train_batch_size=config['batch_size'],\n        num_epochs=config['num_epochs'],\n        embed_size=config['embedding_size'],\n        hidden_size=config['hidden_size'],\n        learning_rate=config['learning_rate'],\n        dropout=config['dropout'],\n        num_layers=config['num_layers']\n    )\nexcept Exception as e:\n    print(f\"\u274c Training failed: {e}\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Inference"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Run inference\ntry:\n    from training.predict import predict\n\n    # Load the best model\n    model_path = os.path.join(output_dir, f\"{config['model_type']}_model_best.pt\")\n\n    # Example usage\n    context = \"\"\"The quick brown fox jumps over the lazy dog. The dog was sleeping in the sun.\"\"\"\n    question = \"What did the fox do?\"\n\n    answer = predict(\n        model_path=model_path,\n        question=question,\n        context=context,\n        model_type=config['model_type']\n    )\n\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\")\nexcept Exception as e:\n    print(f\"\u274c Inference failed: {e}\")"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}